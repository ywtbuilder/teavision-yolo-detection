# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license

# YOLO26-Fusion: Tea Bud Detection model combining TWO research papers
# This model fuses improvements from:
#   1. YOLO-TBD paper (Industrial Crops & Products 226, 2025)
#   2. BiFormer paper (CVPR 2023)
#
# Design Philosophy:
#   - Shallow layers (P2/P3): Use TBAM - enhances local texture/color features
#   - Deep layers (P4/P5): Use BRA - global semantic modeling, long-range dependencies
#   - P2 feature fusion - preserves fine-grained details for small object detection
#
# Combined Improvements:
#   1. TBAM (Triple-Branch Attention) - enhances local feature representation
#   2. BRA (Bi-Level Routing Attention) - efficient global context with O((HW)^{4/3})
#   3. C2fTBAM - integrates SCGC for enlarged receptive field
#   4. C2fBRA - integrates BRA for query-adaptive sparse attention
#   5. P2 fusion - 4-scale detection for small tea buds
#
# Usage:
#   from ultralytics import YOLO
#   model = YOLO('yolo26-fusion.yaml')
#   model.train(data='tea.yaml', epochs=150, imgsz=640)

# Parameters
nc: 80  # number of classes (modify to your dataset, e.g., 1 for tea bud)
end2end: True  # whether to use end-to-end mode
reg_max: 1  # DFL bins
scales: # model compound scaling constants
  # [depth, width, max_channels]
  n: [0.50, 0.25, 1024]  # YOLO26-Fusion-N
  s: [0.50, 0.50, 1024]  # YOLO26-Fusion-S (recommended for comparison with yolo26s)
  m: [0.50, 1.00, 512]   # YOLO26-Fusion-M
  l: [1.00, 1.00, 512]   # YOLO26-Fusion-L
  x: [1.00, 1.50, 512]   # YOLO26-Fusion-X

# YOLO26-Fusion backbone: TBAM (shallow) + BRA (deep)
backbone:
  # [from, repeats, module, args]
  # === Shallow layers: TBAM for local texture/color enhancement ===
  - [-1, 1, Conv, [64, 3, 2]]             # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]            # 1-P2/4
  - [-1, 2, C3k2, [256, False, 0.25]]     # 2
  - [-1, 1, TBAM, [256]]                  # 3 <- TBAM (shallow, local features)
  - [-1, 1, Conv, [256, 3, 2]]            # 4-P3/8
  - [-1, 2, C3k2, [512, False, 0.25]]     # 5
  - [-1, 1, TBAM, [512]]                  # 6 <- TBAM (shallow, local features)
  
  # === Deep layers: BRA for global semantic modeling ===
  - [-1, 1, Conv, [512, 3, 2]]            # 7-P4/16
  - [-1, 2, C2fBRA, [512, True, 8, 4]]    # 8 <- C2fBRA (deep, global context)
  - [-1, 1, Conv, [1024, 3, 2]]           # 9-P5/32
  - [-1, 2, C2fBRA, [1024, True, 4, 4]]   # 10 <- C2fBRA (deep, global context)
  - [-1, 1, SPPF, [1024, 5, 3, True]]     # 11
  - [-1, 2, C2PSA, [1024]]                # 12

# YOLO26-Fusion head: Hybrid C2fTBAM (shallow) + C2fBRA (deep) + P2 fusion
head:
  # === Top-down path ===
  # Deep features: use C2fBRA for global context
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 8], 1, Concat, [1]]             # cat backbone P4 (layer 8)
  - [-1, 2, C2fBRA, [512, True, 8, 4]]    # 15 <- C2fBRA (deep features)

  # Mid features: transition zone
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 5], 1, Concat, [1]]             # cat backbone P3 (layer 5)
  - [-1, 2, C2fTBAM, [256, True]]         # 18 (P3/8-small) <- C2fTBAM (local features)

  # Shallow features: use C2fTBAM for local texture/color
  # P2 feature fusion (improvement for small object detection)
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 2], 1, Concat, [1]]             # cat backbone P2 (layer 2)
  - [-1, 2, C3k2, [128, True]]            # 21 (P2/4-xsmall) <- new detection layer

  # === Bottom-up path ===
  - [-1, 1, Conv, [128, 3, 2]]
  - [[-1, 18], 1, Concat, [1]]            # cat head P3
  - [-1, 2, C2fTBAM, [256, True]]         # 24 (P3/8-small) <- C2fTBAM

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 15], 1, Concat, [1]]            # cat head P4
  - [-1, 2, C2fBRA, [512, True, 8, 4]]    # 27 (P4/16-medium) <- C2fBRA

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]            # cat head P5
  - [-1, 1, C3k2, [1024, True, 0.5, True]] # 30 (P5/32-large)

  # Detect head: P2, P3, P4, P5 (4-scale detection for small tea buds)
  - [[21, 24, 27, 30], 1, Detect, [nc]]   # Detect(P2, P3, P4, P5)
